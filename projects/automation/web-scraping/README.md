# ç½‘ç»œçˆ¬è™«å®Œæ•´æŒ‡å—

> ğŸ•·ï¸ ä»åŸºç¡€åˆ°é«˜çº§çš„ç³»ç»ŸåŒ–ç½‘ç»œçˆ¬è™«å­¦ä¹ é¡¹ç›®

## ğŸ¯ é¡¹ç›®ç®€ä»‹

è¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„ç½‘ç»œçˆ¬è™«å­¦ä¹ é¡¹ç›®ï¼Œæ¶µç›–ä»åŸºç¡€çš„ requests + BeautifulSoup åˆ°é«˜çº§çš„ Scrapyã€Selenium å’Œå¼‚æ­¥çˆ¬è™«æŠ€æœ¯ã€‚åŒ…å«å®é™…æ¡ˆä¾‹ã€æœ€ä½³å®è·µå’Œååçˆ¬ç­–ç•¥ã€‚

## ğŸ“š å­¦ä¹ å†…å®¹

### æ ¸å¿ƒæ¨¡å—

1. **åŸºç¡€çˆ¬è™«** (`01_basic_scraper.py`)
   - HTTP è¯·æ±‚å’Œå“åº”å¤„ç†
   - BeautifulSoup è§£æ HTML
   - CSS é€‰æ‹©å™¨ä½¿ç”¨
   - è¡¨æ ¼æ•°æ®æå–
   - åˆ†é¡µå¤„ç†
   - æ•°æ®ä¿å­˜ï¼ˆJSONã€CSVï¼‰

2. **é«˜çº§çˆ¬è™«** (`02_advanced_scraper.py`)
   - Scrapy æ¡†æ¶ä½¿ç”¨
   - Selenium å¤„ç†åŠ¨æ€å†…å®¹
   - å¼‚æ­¥çˆ¬è™«ï¼ˆaiohttp + asyncioï¼‰
   - æ•°æ®æ¸…æ´—å’Œå¤„ç†
   - ååçˆ¬ç­–ç•¥
   - ä»£ç†å’Œè¯·æ±‚ä¼ªè£…

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å®‰è£…ä¾èµ–

```bash
# åŸºç¡€ä¾èµ–
pip install requests beautifulsoup4 lxml

# é«˜çº§ä¾èµ–ï¼ˆå¯é€‰ï¼‰
pip install scrapy selenium aiohttp asyncio
pip install aiofiles  # å¼‚æ­¥æ–‡ä»¶æ“ä½œ

# Selenium é¢å¤–å®‰è£…
# 1. ä¸‹è½½ ChromeDriver
# 2. æˆ–ä½¿ç”¨ webdriver-manager è‡ªåŠ¨ç®¡ç†
pip install webdriver-manager
```

### è¿è¡Œç¤ºä¾‹

```bash
# 1. å­¦ä¹ åŸºç¡€çˆ¬è™«
python 01_basic_scraper.py

# 2. å­¦ä¹ é«˜çº§çˆ¬è™«
python 02_advanced_scraper.py
```

## ğŸ“– å­¦ä¹ è·¯å¾„

### ç¬¬ä¸€æ­¥ï¼šåŸºç¡€æ¦‚å¿µ
1. ç†è§£ HTTP åè®®
2. å­¦ä¹  HTML è§£æ
3. æŒæ¡ CSS é€‰æ‹©å™¨
4. å­¦ä¼šæ•°æ®æå–

### ç¬¬äºŒæ­¥ï¼šå®é™…åº”ç”¨
1. å¤„ç†é™æ€ç½‘é¡µ
2. æå–è¡¨æ ¼æ•°æ®
3. å¤„ç†åˆ†é¡µ
4. æ•°æ®å­˜å‚¨

### ç¬¬ä¸‰æ­¥ï¼šé«˜çº§æŠ€æœ¯
1. Scrapy æ¡†æ¶
2. JavaScript å¤„ç†
3. å¼‚æ­¥çˆ¬å–
4. ååçˆ¬ç­–ç•¥

## ğŸ“Š ç¤ºä¾‹è¾“å‡º

è¿è¡Œåä¼šç”Ÿæˆï¼š
- `output/basic_page.html` - åŸå§‹ HTML
- `output/news.json` - æ–°é—»æ•°æ®
- `output/products.json` - äº§å“ä¿¡æ¯
- `output/students.json` - å­¦ç”Ÿæ•°æ®
- `output/all_posts.json` - åˆ†é¡µæ•°æ®
- `output/cleaned_data.json` - æ¸…æ´—åæ•°æ®
- `output/*.json` - å„ç§çˆ¬å–ç»“æœ

## ğŸ’» æ ¸å¿ƒä»£ç ç¤ºä¾‹

### åŸºç¡€çˆ¬è™«

```python
import requests
from bs4 import BeautifulSoup

# å‘é€è¯·æ±‚
url = "https://example.com"
response = requests.get(url)

# è§£æ HTML
soup = BeautifulSoup(response.text, 'html.parser')

# æå–æ•°æ®
title = soup.find('title').text
links = soup.find_all('a')
data = [{"link": link.get('href')} for link in links]
```

### CSS é€‰æ‹©å™¨

```python
# é€‰æ‹©æ‰€æœ‰äº§å“
products = soup.select('.product')

for product in products:
    name = product.select_one('.product-name').text
    price = product.select_one('.product-price').text
    print(f"{name}: {price}")
```

### å¼‚æ­¥çˆ¬è™«

```python
import asyncio
import aiohttp

async def fetch_url(session, url):
    async with session.get(url) as response:
        return await response.text()

async def main():
    urls = ['url1', 'url2', 'url3']
    async with aiohttp.ClientSession() as session:
        tasks = [fetch_url(session, url) for url in urls]
        results = await asyncio.gather(*tasks)

asyncio.run(main())
```

### Scrapy æ¡†æ¶

```python
import scrapy

class ExampleSpider(scrapy.Spider):
    name = 'example'
    start_urls = ['http://example.com']

    def parse(self, response):
        for item in response.css('.item'):
            yield {
                'title': item.css('h2::text').get(),
                'price': item.css('.price::text').get()
            }
```

### Selenium å¤„ç†åŠ¨æ€å†…å®¹

```python
from selenium import webdriver
from selenium.webdriver.common.by import By

driver = webdriver.Chrome()
driver.get('https://example.com')

# ç­‰å¾…å…ƒç´ åŠ è½½
element = driver.find_element(By.CLASS_NAME, 'content')

# æå–æ•°æ®
title = driver.title
paragraphs = driver.find_elements(By.TAG_NAME, 'p')

driver.quit()
```

## ğŸ” æ•°æ®æå–æŠ€å·§

### CSS é€‰æ‹©å™¨é€ŸæŸ¥è¡¨

| é€‰æ‹©å™¨ | ç¤ºä¾‹ | è¯´æ˜ |
|--------|------|------|
| æ ‡ç­¾é€‰æ‹©å™¨ | `h1` | æ‰€æœ‰ h1 æ ‡ç­¾ |
| ç±»é€‰æ‹©å™¨ | `.product` | class="product" çš„å…ƒç´  |
| ID é€‰æ‹©å™¨ | `#header` | id="header" çš„å…ƒç´  |
| åä»£é€‰æ‹©å™¨ | `.content p` | .content å†…çš„æ‰€æœ‰ p æ ‡ç­¾ |
| å­é€‰æ‹©å™¨ | `.menu > li` | .menu çš„ç›´æ¥å­å…ƒç´  li |
| å±æ€§é€‰æ‹©å™¨ | `a[href]` | å¸¦æœ‰ href å±æ€§çš„ a æ ‡ç­¾ |
| ä¼ªç±» | `a:hover` | é¼ æ ‡æ‚¬åœçš„ a æ ‡ç­¾ |

### BeautifulSoup æ–¹æ³•

```python
# æŸ¥æ‰¾å•ä¸ªå…ƒç´ 
soup.find('div')  # ç¬¬ä¸€ä¸ª div
soup.find(id='content')  # id ä¸º content çš„å…ƒç´ 

# æŸ¥æ‰¾æ‰€æœ‰å…ƒç´ 
soup.find_all('p')  # æ‰€æœ‰ p æ ‡ç­¾

# è·å–æ–‡æœ¬
element.get_text()  # è·å–æ‰€æœ‰æ–‡æœ¬
element.text  # ç®€åŒ–ç‰ˆæœ¬

# è·å–å±æ€§
element.get('href')  # è·å– href å±æ€§
element['src']  # è·å– src å±æ€§
```

## ğŸ›¡ï¸ ååçˆ¬ç­–ç•¥

### 1. è¯·æ±‚å¤´ä¼ªè£…

```python
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
    'Accept-Encoding': 'gzip, deflate',
    'Connection': 'keep-alive',
}
```

### 2. éšæœºå»¶è¿Ÿ

```python
import random
import time

def random_delay(min_sec=1, max_sec=3):
    delay = random.uniform(min_sec, max_sec)
    time.sleep(delay)
```

### 3. ä»£ç†è½®æ¢

```python
proxies = [
    'http://proxy1:port',
    'http://proxy2:port',
    'http://proxy3:port'
]

proxy = random.choice(proxies)
response = requests.get(url, proxies={'http': proxy, 'https': proxy})
```

### 4. ä¼šè¯ä¿æŒ

```python
session = requests.Session()
session.headers.update({'User-Agent': '...'})
response = session.get(url)
```

## ğŸ“Š æ•°æ®å¤„ç†

### æ•°æ®æ¸…æ´—

```python
import re

def clean_price(price_str):
    # æå–æ•°å­—
    numbers = re.findall(r'\d+', price_str)
    return int(''.join(numbers))

def clean_text(text):
    # å»é™¤ç©ºç™½å’Œæ¢è¡Œ
    return ' '.join(text.split())
```

### æ•°æ®å­˜å‚¨

```python
import json
import csv
import pandas as pd

# JSON å­˜å‚¨
with open('data.json', 'w') as f:
    json.dump(data, f, ensure_ascii=False, indent=2)

# CSV å­˜å‚¨
with open('data.csv', 'w', newline='') as f:
    writer = csv.DictWriter(f, fieldnames=data[0].keys())
    writer.writeheader()
    writer.writerows(data)

# Pandas DataFrame
df = pd.DataFrame(data)
df.to_excel('data.xlsx', index=False)
```

## ğŸ”§ é«˜çº§æŠ€å·§

### 1. å¤„ç†éªŒè¯ç 

```python
# OCR è¯†åˆ«ï¼ˆç®€å•éªŒè¯ç ï¼‰
import pytesseract
from PIL import Image

def solve_captcha(image_path):
    image = Image.open(image_path)
    text = pytesseract.image_to_string(image)
    return text.strip()
```

### 2. æ¨¡æ‹Ÿç™»å½•

```python
# ç™»å½•
login_data = {
    'username': 'user',
    'password': 'pass'
}
session.post('https://example.com/login', data=login_data)

# è®¿é—®éœ€è¦ç™»å½•çš„é¡µé¢
response = session.get('https://example.com/profile')
```

### 3. å¤„ç† Cookies

```python
# è®¾ç½® Cookies
cookies = {'session_id': 'abc123'}
response = requests.get(url, cookies=cookies)

# è·å– Cookies
jar = response.cookies
```

### 4. æ–‡ä»¶ä¸‹è½½

```python
def download_file(url, filename):
    response = requests.get(url, stream=True)
    with open(filename, 'wb') as f:
        for chunk in response.iter_content(chunk_size=8192):
            f.write(chunk)
```

## ğŸ› å¸¸è§é—®é¢˜

### Q1: ç¼–ç é—®é¢˜
```python
# æŒ‡å®šç¼–ç 
response.encoding = 'utf-8'
content = response.text

# æˆ–ä½¿ç”¨äºŒè¿›åˆ¶æ¨¡å¼
content = response.content.decode('utf-8')
```

### Q2: SSL è¯ä¹¦é”™è¯¯
```python
# å¿½ç•¥ SSL éªŒè¯ï¼ˆä»…å¼€å‘ç¯å¢ƒï¼‰
response = requests.get(url, verify=False)
```

### Q3: è¶…æ—¶å¤„ç†
```python
# è®¾ç½®è¶…æ—¶
response = requests.get(url, timeout=10)
```

### Q4: é™æµå¤„ç†
```python
# æ·»åŠ é‡è¯•æœºåˆ¶
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

session = requests.Session()
retry = Retry(total=3, backoff_factor=1)
adapter = HTTPAdapter(max_retries=retry)
session.mount('http://', adapter)
```

## ğŸ“š å­¦ä¹ èµ„æº

### å®˜æ–¹æ–‡æ¡£
- [Requests æ–‡æ¡£](https://docs.python-requests.org/)
- [BeautifulSoup æ–‡æ¡£](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
- [Scrapy æ–‡æ¡£](https://docs.scrapy.org/)
- [Selenium æ–‡æ¡£](https://selenium-python.readthedocs.io/)

### æ¨èå·¥å…·
- **Chrome DevTools**: åˆ†æç½‘é¡µç»“æ„
- **XPath Helper**: XPath æµ‹è¯•å·¥å…·
- **Postman**: API æµ‹è¯•
- **Scrapy Shell**: äº¤äº’å¼è°ƒè¯•

### å®è·µç½‘ç«™
- **httpbin.org**: æµ‹è¯• HTTP è¯·æ±‚
- **jsonplaceholder.typicode.com**: å‡æ•°æ® API
- **quotes.toscrape.com**: ç»ƒä¹ çˆ¬å–
- **books.toscrape.com**: ä¹¦ç±ä¿¡æ¯çˆ¬å–

## âš ï¸ çˆ¬è™«ä¼¦ç†

### éµå®ˆ robots.txt
```python
import urllib.robotparser

def can_fetch(url, user_agent='*'):
    rp = urllib.robotparser.RobotFileParser()
    rp.set_url(f"{urllib.parse.urljoin(url, '/robots.txt')}")
    rp.read()
    return rp.can_fetch(user_agent, url)
```

### æœ€ä½³å®è·µ
1. éµå®ˆç½‘ç«™ robots.txt
2. æ§åˆ¶è¯·æ±‚é¢‘ç‡ï¼Œé¿å…ç»™æœåŠ¡å™¨é€ æˆå‹åŠ›
3. ä¸è¦çˆ¬å–æ•æ„Ÿæˆ–ç§äººä¿¡æ¯
4. å°Šé‡ç‰ˆæƒï¼Œæ³¨æ˜æ•°æ®æ¥æº
5. ä¸è¦ç ´åç½‘ç«™æ­£å¸¸è¿è¡Œ
6. éµå®ˆç›¸å…³æ³•å¾‹æ³•è§„

## ğŸ† å®æˆ˜é¡¹ç›®å»ºè®®

### é¡¹ç›® 1: æ–°é—»èšåˆå™¨
- çˆ¬å–å¤šä¸ªæ–°é—»ç½‘ç«™
- å»é‡å’Œåˆ†ç±»
- ç”Ÿæˆæ¯æ—¥æ–°é—»æ‘˜è¦

### é¡¹ç›® 2: å•†å“ä»·æ ¼ç›‘æ§
- çˆ¬å–ç”µå•†å¹³å°ä»·æ ¼
- ä»·æ ¼å˜åŠ¨æé†’
- ä»·æ ¼è¶‹åŠ¿åˆ†æ

### é¡¹ç›® 3: æ‹›è˜æ•°æ®åˆ†æ
- çˆ¬å–æ‹›è˜ç½‘ç«™èŒä½
- æŠ€èƒ½éœ€æ±‚åˆ†æ
- è–ªèµ„è¶‹åŠ¿ç»Ÿè®¡

### é¡¹ç›® 4: èˆ†æƒ…ç›‘æ§ç³»ç»Ÿ
- çˆ¬å–ç¤¾äº¤åª’ä½“å†…å®¹
- æƒ…æ„Ÿåˆ†æ
- çƒ­ç‚¹è¯é¢˜è¿½è¸ª

## ğŸ¤ è´¡çŒ®

æ¬¢è¿è´¡çŒ®æ›´å¤šç¤ºä¾‹å’Œæœ€ä½³å®è·µï¼

## ğŸ“„ è®¸å¯è¯

MIT License

## ğŸ‘¨â€ğŸ’» ä½œè€…

Claude - Anthropic

---

**å¼€å§‹ä½ çš„çˆ¬è™«ä¹‹æ—…ï¼** ğŸš€

è®°ä½ï¼šæŠ€æœ¯æ— ç½ªï¼Œè¯·åˆç†ä½¿ç”¨ï¼Œéµå®ˆæ³•å¾‹å’Œé“å¾·è§„èŒƒï¼
