"""
é«˜çº§ç½‘ç»œçˆ¬è™«ç¤ºä¾‹
ä½¿ç”¨ Scrapyã€Selenium å’Œå…¶ä»–é«˜çº§æŠ€æœ¯
"""

import time
import json
import csv
from pathlib import Path
from typing import List, Dict
import logging
import random

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


# ============================================================================
# ç¤ºä¾‹ 1: ä½¿ç”¨ Scrapy æ¡†æ¶ï¼ˆä¼ªä»£ç ç¤ºä¾‹ï¼‰
# ============================================================================

def scrapy_example():
    """Scrapy æ¡†æ¶ç¤ºä¾‹ï¼ˆä¼ªä»£ç ï¼‰"""
    print("=" * 60)
    print("ç¤ºä¾‹ 1: Scrapy æ¡†æ¶ä½¿ç”¨")
    print("=" * 60)

    print("""
Scrapy æ˜¯ä¸€ä¸ªå¼ºå¤§çš„ Python çˆ¬è™«æ¡†æ¶ã€‚ä»¥ä¸‹æ˜¯åŸºæœ¬ä½¿ç”¨ç¤ºä¾‹ï¼š

1. å®‰è£… Scrapy:
   pip install scrapy

2. åˆ›å»º Scrapy é¡¹ç›®:
   scrapy startproject myproject

3. åˆ›å»ºçˆ¬è™«:
   scrapy genspider example example.com

4. Scrapy çˆ¬è™«ä»£ç ç¤ºä¾‹:

import scrapy

class ExampleSpider(scrapy.Spider):
    name = 'example'
    start_urls = ['http://example.com']

    def parse(self, response):
        # æå–æ ‡é¢˜
        title = response.css('title::text').get()

        # æå–æ‰€æœ‰é“¾æ¥
        links = response.css('a::attr(href)').getall()

        yield {
            'title': title,
            'links': links
        }

        # è·Ÿéšé“¾æ¥
        for link in links:
            yield response.follow(link, self.parse)

5. è¿è¡Œçˆ¬è™«:
   scrapy crawl example -o output.json
    """)

    # æ¨¡æ‹Ÿ Scrapy è¾“å‡º
    sample_data = {
        "spider_name": "example",
        "items_scraped": 100,
        "status": "completed",
        "output_file": "output/scrapy_results.json"
    }

    output_dir = Path("output")
    output_dir.mkdir(exist_ok=True)

    with open(output_dir / "scrapy_example.json", "w", encoding="utf-8") as f:
        json.dump(sample_data, f, ensure_ascii=False, indent=2)
    print(f"\nâœ“ Scrapy ç¤ºä¾‹é…ç½®å·²ä¿å­˜åˆ°: {output_dir / 'scrapy_example.json'}")


# ============================================================================
# ç¤ºä¾‹ 2: ä½¿ç”¨ Selenium å¤„ç†åŠ¨æ€å†…å®¹
# ============================================================================

def selenium_example():
    """Selenium ç¤ºä¾‹ï¼ˆä¼ªä»£ç ï¼‰"""
    print("\n" + "=" * 60)
    print("ç¤ºä¾‹ 2: Selenium å¤„ç†åŠ¨æ€å†…å®¹")
    print("=" * 60)

    print("""
Selenium ç”¨äºå¤„ç† JavaScript æ¸²æŸ“çš„åŠ¨æ€ç½‘é¡µï¼š

1. å®‰è£… Selenium:
   pip install selenium

2. ä¸‹è½½æµè§ˆå™¨é©±åŠ¨ï¼ˆChromeDriverï¼‰

3. Selenium ä»£ç ç¤ºä¾‹:

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# å¯åŠ¨æµè§ˆå™¨
driver = webdriver.Chrome()
driver.get('https://example.com')

# ç­‰å¾…å…ƒç´ åŠ è½½
element = WebDriverWait(driver, 10).until(
    EC.presence_of_element_located((By.CLASS_NAME, 'content'))
)

# æå–æ•°æ®
title = driver.find_element(By.TAG_NAME, 'h1').text
paragraphs = driver.find_elements(By.TAG_NAME, 'p')

data = {
    'title': title,
    'content': [p.text for p in paragraphs]
}

# å…³é—­æµè§ˆå™¨
driver.quit()
    """)

    # æ¨¡æ‹Ÿ Selenium è¾“å‡º
    sample_data = {
        "browser": "Chrome",
        "page_title": "ç¤ºä¾‹é¡µé¢",
        "elements_found": 15,
        "dynamic_content": True,
        "wait_time": 5.2,
        "data": {
            "title": "åŠ¨æ€åŠ è½½çš„å†…å®¹",
            "paragraphs": ["æ®µè½1", "æ®µè½2", "æ®µè½3"],
            "images": 5,
            "links": 10
        }
    }

    output_dir = Path("output")
    output_dir.mkdir(exist_ok=True)

    with open(output_dir / "selenium_results.json", "w", encoding="utf-8") as f:
        json.dump(sample_data, f, ensure_ascii=False, indent=2)
    print(f"\nâœ“ Selenium ç¤ºä¾‹ç»“æœå·²ä¿å­˜åˆ°: {output_dir / 'selenium_results.json'}")


# ============================================================================
# ç¤ºä¾‹ 3: å¼‚æ­¥çˆ¬è™«ï¼ˆaiohttp + asyncioï¼‰
# ============================================================================

def async_scraper_example():
    """å¼‚æ­¥çˆ¬è™«ç¤ºä¾‹"""
    print("\n" + "=" * 60)
    print("ç¤ºä¾‹ 3: å¼‚æ­¥çˆ¬è™«")
    print("=" * 60)

    import asyncio
    import aiohttp

    print("""
å¼‚æ­¥çˆ¬è™«å¯ä»¥å¤§å¹…æé«˜çˆ¬å–æ•ˆç‡ï¼š

å¼‚æ­¥çˆ¬è™«ä»£ç ç¤ºä¾‹:

import asyncio
import aiohttp

async def fetch_url(session, url):
    async with session.get(url) as response:
        return await response.text()

async def main():
    urls = ['url1', 'url2', 'url3']
    async with aiohttp.ClientSession() as session:
        tasks = [fetch_url(session, url) for url in urls]
        results = await asyncio.gather(*tasks)

asyncio.run(main())
    """)

    # æ¨¡æ‹Ÿå¼‚æ­¥çˆ¬å–ç»“æœ
    print("\næ¨¡æ‹Ÿå¼‚æ­¥çˆ¬å–è¿‡ç¨‹...")

    async def simulate_scraping():
        """æ¨¡æ‹Ÿå¼‚æ­¥çˆ¬å–"""
        urls = [
            "https://api.example.com/data/1",
            "https://api.example.com/data/2",
            "https://api.example.com/data/3",
            "https://api.example.com/data/4",
            "https://api.example.com/data/5"
        ]

        results = []
        for url in urls:
            # æ¨¡æ‹Ÿç½‘ç»œè¯·æ±‚
            await asyncio.sleep(random.uniform(0.1, 0.5))
            result = {
                "url": url,
                "status": "success",
                "data": f"æ•°æ®æ¥è‡ª {url.split('/')[-1]}",
                "timestamp": time.time()
            }
            results.append(result)
            logger.info(f"å®Œæˆçˆ¬å–: {url}")

        return results

    # è¿è¡Œæ¨¡æ‹Ÿ
    start_time = time.time()
    try:
        # æ£€æŸ¥æ˜¯å¦åœ¨äº‹ä»¶å¾ªç¯ä¸­
        loop = asyncio.get_event_loop()
        if loop.is_running():
            # å¦‚æœå·²æœ‰å¾ªç¯ï¼Œè¿è¡Œåœ¨æ–°çº¿ç¨‹ä¸­
            import concurrent.futures
            with concurrent.futures.ThreadPoolExecutor() as executor:
                future = executor.submit(asyncio.run, simulate_scraping())
                results = future.result()
        else:
            results = loop.run_until_complete(simulate_scraping())
    except RuntimeError:
        # å¦‚æœæ²¡æœ‰å¾ªç¯ï¼Œåˆ›å»ºæ–°çš„
        results = asyncio.run(simulate_scraping())

    elapsed_time = time.time() - start_time

    # ä¿å­˜ç»“æœ
    output_dir = Path("output")
    output_dir.mkdir(exist_ok=True)

    with open(output_dir / "async_results.json", "w", encoding="utf-8") as f:
        json.dump({
            "total_urls": len(results),
            "elapsed_time": elapsed_time,
            "avg_time_per_url": elapsed_time / len(results),
            "results": results
        }, f, ensure_ascii=False, indent=2)

    print(f"\nâœ“ å¼‚æ­¥çˆ¬å–ç»“æœå·²ä¿å­˜åˆ°: {output_dir / 'async_results.json'}")
    print(f"çˆ¬å– {len(results)} ä¸ª URL ç”¨æ—¶: {elapsed_time:.2f} ç§’")


# ============================================================================
# ç¤ºä¾‹ 4: æ•°æ®æ¸…æ´—å’Œå¤„ç†
# ============================================================================

def data_cleaning_example():
    """æ•°æ®æ¸…æ´—ç¤ºä¾‹"""
    print("\n" + "=" * 60)
    print("ç¤ºä¾‹ 4: æ•°æ®æ¸…æ´—å’Œå¤„ç†")
    print("=" * 60)

    # æ¨¡æ‹Ÿçˆ¬å–çš„åŸå§‹æ•°æ®ï¼ˆåŒ…å«å™ªéŸ³ï¼‰
    raw_data = [
        {"title": "  è‹¹æœ iPhone 15 Pro  ", "price": "Â¥8,999", "rating": "4.8/5"},
        {"title": "åä¸º Mate 60 Pro", "price": "Â¥6999", "rating": " 4.6 / 5 "},
        {"title": "å°ç±³14 Ultra", "price": "ï¿¥5999", "rating": "4.7/5"},
        {"title": "ä¸‰æ˜Ÿ Galaxy S24", "price": "Â¥7999", "rating": "4.5 /5"},
        {"title": "   OPPO Find X7   ", "price": "ï¿¥4999", "rating": "4.4/5"}
    ]

    print("åŸå§‹æ•°æ®:")
    for item in raw_data:
        print(f"  {item}")

    # æ•°æ®æ¸…æ´—
    cleaned_data = []
    for item in raw_data:
        cleaned_item = {
            # æ¸…ç†æ ‡é¢˜ï¼ˆå»é™¤ç©ºæ ¼ï¼‰
            "title": item["title"].strip(),
            # æ¸…ç†ä»·æ ¼ï¼ˆç»Ÿä¸€æ ¼å¼ï¼Œå»é™¤ç¬¦å·ï¼‰
            "price_numeric": int(item["price"].replace("Â¥", "").replace("ï¿¥", "").replace(",", "")),
            # æ¸…ç†è¯„åˆ†ï¼ˆæå–æ•°å­—ï¼Œå¤„ç† "4.6 / 5" å’Œ "4.8/5" æ ¼å¼ï¼‰
            "rating_numeric": float(item["rating"].split('/')[0].strip())
        }
        cleaned_data.append(cleaned_item)

    print("\næ¸…æ´—åçš„æ•°æ®:")
    for item in cleaned_data:
        print(f"  {item}")

    # æ•°æ®å¤„ç†å’Œåˆ†æ
    print("\næ•°æ®åˆ†æ:")
    avg_price = sum(item["price_numeric"] for item in cleaned_data) / len(cleaned_data)
    avg_rating = sum(item["rating_numeric"] for item in cleaned_data) / len(cleaned_data)

    print(f"  å¹³å‡ä»·æ ¼: Â¥{avg_price:.2f}")
    print(f"  å¹³å‡è¯„åˆ†: {avg_rating:.2f}")

    # æ’åº
    sorted_by_price = sorted(cleaned_data, key=lambda x: x["price_numeric"])
    print(f"\nä»·æ ¼æ’åºï¼ˆä»ä½åˆ°é«˜ï¼‰:")
    for item in sorted_by_price:
        print(f"  {item['title']}: Â¥{item['price_numeric']}")

    # ä¿å­˜æ¸…æ´—åçš„æ•°æ®
    output_dir = Path("output")
    output_dir.mkdir(exist_ok=True)

    with open(output_dir / "cleaned_data.json", "w", encoding="utf-8") as f:
        json.dump(cleaned_data, f, ensure_ascii=False, indent=2)

    # ä¿å­˜ä¸º CSV
    csv_file = output_dir / "cleaned_data.csv"
    with open(csv_file, "w", newline="", encoding="utf-8") as f:
        if cleaned_data:
            writer = csv.DictWriter(f, fieldnames=cleaned_data[0].keys())
            writer.writeheader()
            writer.writerows(cleaned_data)

    print(f"\nâœ“ æ¸…æ´—åæ•°æ®å·²ä¿å­˜:")
    print(f"  JSON: {output_dir / 'cleaned_data.json'}")
    print(f"  CSV: {csv_file}")


# ============================================================================
# ç¤ºä¾‹ 5: ååçˆ¬ç­–ç•¥
# ============================================================================

def anti_anti_crawler_example():
    """ååçˆ¬ç­–ç•¥ç¤ºä¾‹"""
    print("\n" + "=" * 60)
    print("ç¤ºä¾‹ 5: ååçˆ¬ç­–ç•¥")
    print("=" * 60)

    print("""
å¸¸è§çš„ååçˆ¬ç­–ç•¥ï¼š

1. User-Agent è½®æ¢:
   - ä½¿ç”¨ä¸åŒçš„æµè§ˆå™¨æ ‡è¯†

2. è¯·æ±‚å¤´ä¼ªè£…:
   - æ·»åŠ  Acceptã€Accept-Language ç­‰

3. ä»£ç† IP:
   - ä½¿ç”¨ä»£ç†æ± è½®æ¢ IP

4. è¯·æ±‚é¢‘ç‡æ§åˆ¶:
   - æ·»åŠ éšæœºå»¶è¿Ÿ
   - é¿å…è¯·æ±‚è¿‡å¿«

5. ä¼šè¯ä¿æŒ:
   - ä½¿ç”¨ cookies å’Œ sessions

6. éªŒè¯ç å¤„ç†:
   - OCR è¯†åˆ«
   - æ‰“ç å¹³å°

7. åˆ†å¸ƒå¼çˆ¬å–:
   - ä½¿ç”¨å¤šå°æœºå™¨
   - è´Ÿè½½å‡è¡¡
    """)

    # æ¨¡æ‹Ÿå®ç°ï¼šéšæœºå»¶è¿Ÿ
    def random_delay(min_sec=1, max_sec=3):
        """éšæœºå»¶è¿Ÿ"""
        delay = random.uniform(min_sec, max_sec)
        logger.info(f"ç­‰å¾… {delay:.2f} ç§’...")
        time.sleep(delay)

    # æ¨¡æ‹Ÿçˆ¬å–
    print("\næ¨¡æ‹Ÿçˆ¬å–è¿‡ç¨‹ï¼ˆå¸¦ååçˆ¬ç­–ç•¥ï¼‰:")

    urls = [
        "https://example.com/page1",
        "https://example.com/page2",
        "https://example.com/page3"
    ]

    results = []
    for i, url in enumerate(urls, 1):
        logger.info(f"æ­£åœ¨çˆ¬å– {i}/{len(urls)}: {url}")

        # éšæœº User-Agent
        user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36",
            "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36"
        ]
        user_agent = random.choice(user_agents)

        # æ¨¡æ‹Ÿè¯·æ±‚
        random_delay(0.5, 1.5)

        result = {
            "url": url,
            "status": "success",
            "user_agent": user_agent,
            "timestamp": time.time(),
            "data_length": random.randint(100, 1000)
        }
        results.append(result)
        logger.info(f"çˆ¬å–å®Œæˆï¼Œè·å– {result['data_length']} å­—èŠ‚æ•°æ®")

    # ä¿å­˜ç»“æœ
    output_dir = Path("output")
    output_dir.mkdir(exist_ok=True)

    with open(output_dir / "anti_crawler_results.json", "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print(f"\nâœ“ ååçˆ¬çˆ¬å–ç»“æœå·²ä¿å­˜åˆ°: {output_dir / 'anti_crawler_results.json'}")


if __name__ == "__main__":
    print("\nğŸ•·ï¸  é«˜çº§ç½‘ç»œçˆ¬è™«ç¤ºä¾‹\n")

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    Path("output").mkdir(exist_ok=True)

    # è¿è¡Œæ‰€æœ‰ç¤ºä¾‹
    scrapy_example()
    selenium_example()
    async_scraper_example()
    data_cleaning_example()
    anti_anti_crawler_example()

    print("\n" + "=" * 60)
    print("âœ… æ‰€æœ‰é«˜çº§çˆ¬è™«ç¤ºä¾‹å®Œæˆï¼")
    print("=" * 60)
    print("\nğŸ’¡ æ¥ä¸‹æ¥å¯ä»¥å°è¯•ï¼š")
    print("1. å®‰è£… scrapy: pip install scrapy")
    print("2. å®‰è£… selenium: pip install selenium")
    print("3. å®‰è£… aiohttp: pip install aiohttp")
    print("4. æŸ¥çœ‹ output/ ç›®å½•ä¸­çš„çˆ¬å–ç»“æœ")
    print("5. ç¼–å†™è‡ªå·±çš„çˆ¬è™«é¡¹ç›®")
    print("=" * 60 + "\n")
