"""
åŸºç¡€ç½‘ç»œçˆ¬è™«ç¤ºä¾‹
ä½¿ç”¨ requests å’Œ BeautifulSoup çˆ¬å–ç½‘é¡µæ•°æ®
"""

import requests
from bs4 import BeautifulSoup
import json
import csv
import time
from pathlib import Path
from typing import List, Dict
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


# ============================================================================
# ç¤ºä¾‹ 1: åŸºç¡€ç½‘é¡µçˆ¬å–
# ============================================================================

def basic_web_scraping():
    """åŸºç¡€ç½‘é¡µçˆ¬å–ç¤ºä¾‹"""
    print("=" * 60)
    print("ç¤ºä¾‹ 1: åŸºç¡€ç½‘é¡µçˆ¬å–")
    print("=" * 60)

    # è®¾ç½®è¯·æ±‚å¤´ï¼Œæ¨¡æ‹Ÿæµè§ˆå™¨
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }

    try:
        # å‘é€ HTTP è¯·æ±‚
        url = "https://httpbin.org/html"
        logger.info(f"æ­£åœ¨è®¿é—®: {url}")
        response = requests.get(url, headers=headers, timeout=10)

        # æ£€æŸ¥å“åº”çŠ¶æ€
        logger.info(f"å“åº”çŠ¶æ€ç : {response.status_code}")
        logger.info(f"å“åº”å¤´ Content-Type: {response.headers.get('Content-Type')}")

        if response.status_code == 200:
            # è§£æ HTML
            soup = BeautifulSoup(response.text, 'html.parser')

            # æå–æ ‡é¢˜
            title = soup.find('title')
            if title:
                print(f"ç½‘é¡µæ ‡é¢˜: {title.text}")
            else:
                print("æœªæ‰¾åˆ°æ ‡é¢˜")

            # æå– h1 æ ‡ç­¾
            h1 = soup.find('h1')
            if h1:
                print(f"H1 æ ‡ç­¾å†…å®¹: {h1.text}")

            # æå–æ®µè½
            paragraphs = soup.find_all('p')
            print(f"\næ‰¾åˆ° {len(paragraphs)} ä¸ªæ®µè½:")
            for i, p in enumerate(paragraphs, 1):
                print(f"  æ®µè½ {i}: {p.text[:100]}...")

            # ä¿å­˜ HTML
            output_dir = Path("output")
            output_dir.mkdir(exist_ok=True)

            with open(output_dir / "basic_page.html", "w", encoding="utf-8") as f:
                f.write(response.text)
            print(f"\nâœ“ HTML å·²ä¿å­˜åˆ°: {output_dir / 'basic_page.html'}")

        else:
            logger.error(f"è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")

    except requests.exceptions.RequestException as e:
        logger.error(f"è¯·æ±‚å¼‚å¸¸: {e}")


# ============================================================================
# ç¤ºä¾‹ 2: çˆ¬å–æ–°é—»åˆ—è¡¨
# ============================================================================

def scrape_news_list():
    """çˆ¬å–æ–°é—»åˆ—è¡¨ç¤ºä¾‹"""
    print("\n" + "=" * 60)
    print("ç¤ºä¾‹ 2: çˆ¬å–æ–°é—»åˆ—è¡¨")
    print("=" * 60)

    # ä½¿ç”¨ç¤ºä¾‹æ–°é—»ç½‘ç«™ï¼ˆJSONPlaceholderï¼‰
    url = "https://jsonplaceholder.typicode.com/posts"

    try:
        # è·å– JSON æ•°æ®
        logger.info(f"æ­£åœ¨è·å–æ–°é—»æ•°æ®: {url}")
        response = requests.get(url, timeout=10)
        response.raise_for_status()

        # è§£æ JSON
        news_list = response.json()
        logger.info(f"è·å–åˆ° {len(news_list)} æ¡æ–°é—»")

        # å¤„ç†å‰ 5 æ¡æ–°é—»
        news_data = []
        for news in news_list[:5]:
            news_item = {
                "id": news["id"],
                "title": news["title"],
                "body": news["body"],
                "user_id": news["userId"]  # API è¿”å›çš„æ˜¯ userId
            }
            news_data.append(news_item)

        # ä¿å­˜ä¸º JSON
        output_dir = Path("output")
        output_dir.mkdir(exist_ok=True)

        with open(output_dir / "news.json", "w", encoding="utf-8") as f:
            json.dump(news_data, f, ensure_ascii=False, indent=2)
        print(f"âœ“ æ–°é—»æ•°æ®å·²ä¿å­˜åˆ°: {output_dir / 'news.json'}")

        # ä¿å­˜ä¸º CSV
        csv_file = output_dir / "news.csv"
        with open(csv_file, "w", newline="", encoding="utf-8") as f:
            if news_data:
                writer = csv.DictWriter(f, fieldnames=news_data[0].keys())
                writer.writeheader()
                writer.writerows(news_data)
        print(f"âœ“ æ–°é—»æ•°æ®å·²ä¿å­˜åˆ°: {csv_file}")

        # æ˜¾ç¤ºæ–°é—»
        print("\næ–°é—»åˆ—è¡¨:")
        for news in news_data:
            print(f"  ID: {news['id']}")
            print(f"  æ ‡é¢˜: {news['title']}")
            print(f"  å†…å®¹: {news['body'][:100]}...")
            print()

    except requests.exceptions.RequestException as e:
        logger.error(f"è¯·æ±‚å¼‚å¸¸: {e}")


# ============================================================================
# ç¤ºä¾‹ 3: ä½¿ç”¨ CSS é€‰æ‹©å™¨
# ============================================================================

def css_selector_example():
    """CSS é€‰æ‹©å™¨ç¤ºä¾‹"""
    print("\n" + "=" * 60)
    print("ç¤ºä¾‹ 3: CSS é€‰æ‹©å™¨ä½¿ç”¨")
    print("=" * 60)

    # åˆ›å»ºç¤ºä¾‹ HTML
    html_content = """
    <!DOCTYPE html>
    <html>
    <head>
        <title>äº§å“åˆ—è¡¨</title>
    </head>
    <body>
        <div class="product">
            <h2 class="product-name">äº§å“ A</h2>
            <p class="product-price">ä»·æ ¼: Â¥99</p>
            <p class="product-desc">è¿™æ˜¯äº§å“ A çš„æè¿°</p>
        </div>
        <div class="product">
            <h2 class="product-name">äº§å“ B</h2>
            <p class="product-price">ä»·æ ¼: Â¥199</p>
            <p class="product-desc">è¿™æ˜¯äº§å“ B çš„æè¿°</p>
        </div>
        <div class="product">
            <h2 class="product-name">äº§å“ C</h2>
            <p class="product-price">ä»·æ ¼: Â¥299</p>
            <p class="product-desc">è¿™æ˜¯äº§å“ C çš„æè¿°</p>
        </div>
    </body>
    </html>
    """

    # è§£æ HTML
    soup = BeautifulSoup(html_content, 'html.parser')

    # ä½¿ç”¨ CSS é€‰æ‹©å™¨
    print("ä½¿ç”¨ CSS é€‰æ‹©å™¨æå–æ•°æ®:\n")

    # é€‰æ‹©æ‰€æœ‰äº§å“
    products = soup.select('.product')
    print(f"æ‰¾åˆ° {len(products)} ä¸ªäº§å“\n")

    product_list = []
    for product in products:
        # ä½¿ç”¨ CSS é€‰æ‹©å™¨æå–ä¿¡æ¯
        name = product.select_one('.product-name').text
        price = product.select_one('.product-price').text
        desc = product.select_one('.product-desc').text

        product_info = {
            "name": name,
            "price": price,
            "description": desc
        }
        product_list.append(product_info)

        print(f"äº§å“åç§°: {name}")
        print(f"ä»·æ ¼: {price}")
        print(f"æè¿°: {desc}")
        print("-" * 40)

    # ä¿å­˜æ•°æ®
    output_dir = Path("output")
    output_dir.mkdir(exist_ok=True)

    with open(output_dir / "products.json", "w", encoding="utf-8") as f:
        json.dump(product_list, f, ensure_ascii=False, indent=2)
    print(f"\nâœ“ äº§å“æ•°æ®å·²ä¿å­˜åˆ°: {output_dir / 'products.json'}")


# ============================================================================
# ç¤ºä¾‹ 4: å¤„ç†è¡¨æ ¼æ•°æ®
# ============================================================================

def scrape_table_data():
    """çˆ¬å–è¡¨æ ¼æ•°æ®ç¤ºä¾‹"""
    print("\n" + "=" * 60)
    print("ç¤ºä¾‹ 4: çˆ¬å–è¡¨æ ¼æ•°æ®")
    print("=" * 60)

    # åˆ›å»ºç¤ºä¾‹è¡¨æ ¼ HTML
    html_content = """
    <!DOCTYPE html>
    <html>
    <head>
        <title>å­¦ç”Ÿæˆç»©è¡¨</title>
    </head>
    <body>
        <table id="students">
            <thead>
                <tr>
                    <th>å§“å</th>
                    <th>å¹´é¾„</th>
                    <th>æ•°å­¦æˆç»©</th>
                    <th>è‹±è¯­æˆç»©</th>
                    <th>è¯­æ–‡æˆç»©</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>å¼ ä¸‰</td>
                    <td>18</td>
                    <td>95</td>
                    <td>88</td>
                    <td>92</td>
                </tr>
                <tr>
                    <td>æå››</td>
                    <td>19</td>
                    <td>87</td>
                    <td>90</td>
                    <td>85</td>
                </tr>
                <tr>
                    <td>ç‹äº”</td>
                    <td>18</td>
                    <td>92</td>
                    <td>85</td>
                    <td>90</td>
                </tr>
            </tbody>
        </table>
    </body>
    </html>
    """

    # è§£æ HTML
    soup = BeautifulSoup(html_content, 'html.parser')

    # æ‰¾åˆ°è¡¨æ ¼
    table = soup.find('table')
    if not table:
        print("æœªæ‰¾åˆ°è¡¨æ ¼")
        return

    # æå–è¡¨å¤´
    headers = []
    header_row = table.find('thead')
    if header_row:
        for th in header_row.find_all('th'):
            headers.append(th.text.strip())

    print(f"è¡¨æ ¼åˆ—å: {headers}\n")

    # æå–æ•°æ®è¡Œ
    data_rows = []
    body = table.find('tbody')
    if body:
        for row in body.find_all('tr'):
            row_data = [td.text.strip() for td in row.find_all('td')]
            if row_data:
                data_rows.append(row_data)

    # æ˜¾ç¤ºæ•°æ®
    print("è¡¨æ ¼æ•°æ®:")
    for row in data_rows:
        print(row)

    # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
    students = []
    for row in data_rows:
        student = dict(zip(headers, row))
        students.append(student)

    # è®¡ç®—å¹³å‡åˆ†
    if students:
        print("\nå­¦ç”Ÿå¹³å‡æˆç»©:")
        for student in students:
            math = int(student['æ•°å­¦æˆç»©'])
            english = int(student['è‹±è¯­æˆç»©'])
            chinese = int(student['è¯­æ–‡æˆç»©'])
            avg = (math + english + chinese) / 3
            print(f"{student['å§“å']}: {avg:.1f}")

    # ä¿å­˜æ•°æ®
    output_dir = Path("output")
    output_dir.mkdir(exist_ok=True)

    with open(output_dir / "students.json", "w", encoding="utf-8") as f:
        json.dump(students, f, ensure_ascii=False, indent=2)
    print(f"\nâœ“ å­¦ç”Ÿæ•°æ®å·²ä¿å­˜åˆ°: {output_dir / 'students.json'}")


# ============================================================================
# ç¤ºä¾‹ 5: å¤„ç†åˆ†é¡µ
# ============================================================================

def scrape_paginated_data():
    """å¤„ç†åˆ†é¡µæ•°æ®ç¤ºä¾‹"""
    print("\n" + "=" * 60)
    print("ç¤ºä¾‹ 5: å¤„ç†åˆ†é¡µæ•°æ®")
    print("=" * 60)

    # ä½¿ç”¨åˆ†é¡µ API ç¤ºä¾‹
    base_url = "https://jsonplaceholder.typicode.com/posts"
    all_posts = []
    page = 1

    try:
        while True:
            # æ„å»ºåˆ†é¡µ URL
            url = f"{base_url}?_page={page}&_limit=10"
            logger.info(f"æ­£åœ¨è·å–ç¬¬ {page} é¡µæ•°æ®")

            response = requests.get(url, timeout=10)
            response.raise_for_status()

            # æ£€æŸ¥å“åº”å¤´ä¸­çš„æ€»æ•°é‡
            total_count = response.headers.get('X-Total-Count')
            if total_count:
                total_count = int(total_count)
                logger.info(f"æ€»æ•°æ®é‡: {total_count}")

            # è§£ææ•°æ®
            posts = response.json()
            if not posts:
                logger.info("æ²¡æœ‰æ›´å¤šæ•°æ®ï¼Œåœæ­¢çˆ¬å–")
                break

            all_posts.extend(posts)
            logger.info(f"å½“å‰é¡µæœ‰ {len(posts)} æ¡æ•°æ®ï¼Œç´¯è®¡ {len(all_posts)} æ¡")

            # åªçˆ¬å–å‰ 3 é¡µä½œä¸ºç¤ºä¾‹
            if page >= 3:
                logger.info("è¾¾åˆ°æœ€å¤§é¡µæ•°é™åˆ¶ï¼Œåœæ­¢çˆ¬å–")
                break

            page += 1
            time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«

        logger.info(f"çˆ¬å–å®Œæˆï¼Œæ€»å…±è·å– {len(all_posts)} æ¡æ•°æ®")

        # ä¿å­˜æ•°æ®
        output_dir = Path("output")
        output_dir.mkdir(exist_ok=True)

        with open(output_dir / "all_posts.json", "w", encoding="utf-8") as f:
            json.dump(all_posts, f, ensure_ascii=False, indent=2)
        print(f"âœ“ æ‰€æœ‰æ•°æ®å·²ä¿å­˜åˆ°: {output_dir / 'all_posts.json'}")

        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        print(f"\næ•°æ®ç»Ÿè®¡:")
        print(f"  æ€»æ¡æ•°: {len(all_posts)}")
        print(f"  é¡µæ•°: {page - 1}")
        print(f"  å¹³å‡æ¯é¡µ: {len(all_posts) / (page - 1):.1f} æ¡")

    except requests.exceptions.RequestException as e:
        logger.error(f"è¯·æ±‚å¼‚å¸¸: {e}")


if __name__ == "__main__":
    print("\nğŸ•·ï¸  ç½‘ç»œçˆ¬è™«åŸºç¡€ç¤ºä¾‹\n")

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    Path("output").mkdir(exist_ok=True)

    # è¿è¡Œæ‰€æœ‰ç¤ºä¾‹
    basic_web_scraping()
    scrape_news_list()
    css_selector_example()
    scrape_table_data()
    scrape_paginated_data()

    print("\n" + "=" * 60)
    print("âœ… æ‰€æœ‰çˆ¬è™«ç¤ºä¾‹å®Œæˆï¼")
    print("=" * 60)
    print("\nğŸ’¡ æ¥ä¸‹æ¥å¯ä»¥å°è¯•ï¼š")
    print("1. è¿è¡Œ 02_advanced_scraper.py å­¦ä¹ é«˜çº§çˆ¬è™«æŠ€å·§")
    print("2. æŸ¥çœ‹ output/ ç›®å½•ä¸­çš„çˆ¬å–ç»“æœ")
    print("3. ä¿®æ”¹ä»£ç çˆ¬å–çœŸå®ç½‘ç«™æ•°æ®")
    print("=" * 60 + "\n")
