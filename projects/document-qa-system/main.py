"""
æ–‡æ¡£é—®ç­”ç³»ç»Ÿ - å®æˆ˜é¡¹ç›®
åŸºäº LangChain å’Œ RAGFlow çš„æ™ºèƒ½æ–‡æ¡£é—®ç­”ç³»ç»Ÿ

åŠŸèƒ½ç‰¹æ€§ï¼š
- æ”¯æŒå¤šç§æ–‡æ¡£æ ¼å¼ï¼ˆPDFã€TXTã€DOCXï¼‰
- æ™ºèƒ½æ–‡æœ¬åˆ†å—
- å‘é‡åŒ–å­˜å‚¨å’Œæ£€ç´¢
- å¤šè½®å¯¹è¯
- ç­”æ¡ˆæº¯æº
- Web ç•Œé¢
"""

import os
import sys
from pathlib import Path
from typing import List, Dict, Any, Optional

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent))

# ============================================================================
# é…ç½®ç®¡ç†
# ============================================================================

class Config:
    """ç³»ç»Ÿé…ç½®"""

    # LLM é…ç½®
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
    MODEL_NAME = "gpt-3.5-turbo"
    TEMPERATURE = 0.1

    # æ–‡æ¡£å¤„ç†é…ç½®
    CHUNK_SIZE = 500
    CHUNK_OVERLAP = 50
    MAX_CHUNKS_PER_QUERY = 5

    # æ£€ç´¢é…ç½®
    SIMILARITY_THRESHOLD = 0.7

    # å­˜å‚¨é…ç½®
    VECTOR_STORE_PATH = "./data/vectorstore"
    KNOWLEDGE_BASE_PATH = "./data/knowledge"

    # RAGFlow é…ç½®ï¼ˆå¯é€‰ï¼‰
    RAGFLOW_BASE_URL = os.getenv("RAGFLOW_BASE_URL", "http://localhost:9380")
    RAGFLOW_API_KEY = os.getenv("RAGFLOW_API_KEY", "")

    # Web é…ç½®
    WEB_HOST = "127.0.0.1"
    WEB_PORT = 8000


# ============================================================================
# æ–‡æ¡£å¤„ç†å™¨
# ============================================================================

from langchain_community.document_loaders import (
    TextLoader,
    PyPDFLoader,
    UnstructuredWordDocumentLoader
)
from langchain.text_splitter import RecursiveCharacterTextSplitter

class DocumentProcessor:
    """æ–‡æ¡£å¤„ç†å™¨"""

    @staticmethod
    def load_document(file_path: str) -> List:
        """
        åŠ è½½æ–‡æ¡£

        Args:
            file_path: æ–‡æ¡£è·¯å¾„

        Returns:
            æ–‡æ¡£å†…å®¹åˆ—è¡¨
        """
        file_path = Path(file_path)

        if not file_path.exists():
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

        # æ ¹æ®æ–‡ä»¶æ‰©å±•åé€‰æ‹©åŠ è½½å™¨
        if file_path.suffix.lower() == ".pdf":
            loader = PyPDFLoader(str(file_path))
        elif file_path.suffix.lower() == ".docx":
            loader = UnstructuredWordDocumentLoader(str(file_path))
        elif file_path.suffix.lower() == ".txt":
            loader = TextLoader(str(file_path), encoding="utf-8")
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path.suffix}")

        return loader.load()

    @staticmethod
    def split_text(documents: List, chunk_size: int = 500, chunk_overlap: int = 50) -> List:
        """
        æ–‡æœ¬åˆ†å—

        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            chunk_size: å—å¤§å°
            chunk_overlap: é‡å å¤§å°

        Returns:
            åˆ†å—åçš„æ–‡æ¡£åˆ—è¡¨
        """
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", ".", "!", "?"]
        )
        return text_splitter.split_documents(documents)

    @staticmethod
    def process_file(file_path: str, chunk_size: int = 500, chunk_overlap: int = 50) -> List:
        """
        å®Œæ•´å¤„ç†æ–‡æ¡£

        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            chunk_size: å—å¤§å°
            chunk_overlap: é‡å å¤§å°

        Returns:
            å¤„ç†åçš„æ–‡æ¡£å—
        """
        # åŠ è½½æ–‡æ¡£
        documents = DocumentProcessor.load_document(file_path)

        # æ–‡æœ¬åˆ†å—
        splits = DocumentProcessor.split_text(documents, chunk_size, chunk_overlap)

        # æ·»åŠ å…ƒæ•°æ®
        for doc in splits:
            doc.metadata["source_file"] = Path(file_path).name

        return splits


# ============================================================================
# å‘é‡å­˜å‚¨ç®¡ç†
# ============================================================================

from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma

class VectorStoreManager:
    """å‘é‡å­˜å‚¨ç®¡ç†å™¨"""

    def __init__(self, persist_directory: str):
        """
        åˆå§‹åŒ–å‘é‡å­˜å‚¨ç®¡ç†å™¨

        Args:
            persist_directory: å­˜å‚¨ç›®å½•
        """
        self.persist_directory = persist_directory
        self.embeddings = OpenAIEmbeddings()
        self.vectorstore = None

    def create_vectorstore(self, documents: List) -> Chroma:
        """
        åˆ›å»ºå‘é‡å­˜å‚¨

        Args:
            documents: æ–‡æ¡£åˆ—è¡¨

        Returns:
            å‘é‡å­˜å‚¨å¯¹è±¡
        """
        self.vectorstore = Chroma.from_documents(
            documents=documents,
            embedding=self.embeddings,
            persist_directory=self.persist_directory
        )
        return self.vectorstore

    def load_vectorstore(self) -> Chroma:
        """
        åŠ è½½ç°æœ‰å‘é‡å­˜å‚¨

        Returns:
            å‘é‡å­˜å‚¨å¯¹è±¡
        """
        if os.path.exists(self.persist_directory):
            self.vectorstore = Chroma(
                persist_directory=self.persist_directory,
                embedding_function=self.embeddings
            )
        return self.vectorstore

    def add_documents(self, documents: List) -> None:
        """
        æ·»åŠ æ–‡æ¡£åˆ°å‘é‡å­˜å‚¨

        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
        """
        if self.vectorstore is None:
            self.create_vectorstore(documents)
        else:
            self.vectorstore.add_documents(documents)

    def similarity_search(self, query: str, k: int = 5) -> List:
        """
        ç›¸ä¼¼åº¦æœç´¢

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            k: è¿”å›æ–‡æ¡£æ•°é‡

        Returns:
            ç›¸å…³æ–‡æ¡£åˆ—è¡¨
        """
        if self.vectorstore is None:
            raise ValueError("å‘é‡å­˜å‚¨æœªåˆå§‹åŒ–")

        return self.vectorstore.similarity_search(query, k=k)


# ============================================================================
# RAG é—®ç­”ç³»ç»Ÿ
# ============================================================================

from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory

class DocumentQASystem:
    """æ–‡æ¡£é—®ç­”ç³»ç»Ÿ"""

    def __init__(self, config: Config):
        """
        åˆå§‹åŒ–é—®ç­”ç³»ç»Ÿ

        Args:
            config: é…ç½®å¯¹è±¡
        """
        self.config = config
        self.vectorstore_manager = VectorStoreManager(config.VECTOR_STORE_PATH)
        self.memory = ConversationBufferMemory(
            memory_key="chat_history",
            return_messages=True
        )

        # åˆå§‹åŒ– LLM
        self.llm = ChatOpenAI(
            model=config.MODEL_NAME,
            temperature=config.TEMPERATURE
        )

        # åˆ›å»ºæç¤ºæ¨¡æ¿
        self.prompt = PromptTemplate(
            input_variables=["context", "question"],
            template="""åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ã€‚

ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
{context}

é—®é¢˜ï¼š{question}

è¯·æä¾›å‡†ç¡®ã€è¯¦ç»†çš„å›ç­”ã€‚å¦‚æœä¸Šä¸‹æ–‡æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯´æ˜ã€‚å›ç­”åº”åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡ã€‚"""
        )

        self.qa_chain = None

    def build_qa_chain(self) -> ConversationalRetrievalChain:
        """
        æ„å»ºé—®ç­”é“¾

        Returns:
            é—®ç­”é“¾å¯¹è±¡
        """
        if self.vectorstore_manager.vectorstore is None:
            raise ValueError("å‘é‡å­˜å‚¨æœªåˆå§‹åŒ–")

        self.qa_chain = ConversationalRetrievalChain.from_llm(
            llm=self.llm,
            memory=self.memory,
            retriever=self.vectorstore_manager.vectorstore.as_retriever(
                search_kwargs={"k": self.config.MAX_CHUNKS_PER_QUERY}
            ),
            combine_docs_chain_kwargs={"prompt": self.prompt}
        )

        return self.qa_chain

    def add_knowledge_base(self, file_paths: List[str]) -> None:
        """
        æ·»åŠ çŸ¥è¯†åº“æ–‡æ¡£

        Args:
            file_paths: æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        all_documents = []

        for file_path in file_paths:
            try:
                documents = DocumentProcessor.process_file(
                    file_path,
                    self.config.CHUNK_SIZE,
                    self.config.CHUNK_OVERLAP
                )
                all_documents.extend(documents)
                print(f"âœ“ å·²å¤„ç†: {file_path}")
            except Exception as e:
                print(f"âœ— å¤„ç†å¤±è´¥: {file_path}, é”™è¯¯: {e}")

        # æ·»åŠ åˆ°å‘é‡å­˜å‚¨
        self.vectorstore_manager.add_documents(all_documents)
        print(f"\næ€»å…±å¤„ç†äº† {len(all_documents)} ä¸ªæ–‡æ¡£å—")

    def query(self, question: str) -> Dict[str, Any]:
        """
        é—®ç­”æŸ¥è¯¢

        Args:
            question: é—®é¢˜

        Returns:
            å›ç­”ç»“æœ
        """
        if self.qa_chain is None:
            self.build_qa_chain()

        result = self.qa_chain({"question": question})

        # è·å–ç›¸å…³æ–‡æ¡£
        docs = self.vectorstore_manager.similarity_search(question, k=3)
        sources = [doc.metadata.get("source_file", "æœªçŸ¥æ¥æº") for doc in docs]

        return {
            "answer": result["answer"],
            "sources": list(set(sources)),  # å»é‡
            "chat_history": result.get("chat_history", [])
        }

    def chat(self, question: str) -> str:
        """
        ç®€å•å¯¹è¯ï¼ˆè¿”å›å­—ç¬¦ä¸²ï¼‰

        Args:
            question: é—®é¢˜

        Returns:
            å›ç­”æ–‡æœ¬
        """
        result = self.query(question)
        return result["answer"]


# ============================================================================
# ç¤ºä¾‹ä½¿ç”¨
# ============================================================================

def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºç³»ç»Ÿä½¿ç”¨"""
    print("=" * 60)
    print("ğŸ“š æ–‡æ¡£é—®ç­”ç³»ç»Ÿæ¼”ç¤º")
    print("=" * 60)

    # æ£€æŸ¥ API å¯†é’¥
    if not Config.OPENAI_API_KEY:
        print("âš ï¸  è¯·è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
        print("export OPENAI_API_KEY='your-api-key-here'")
        return

    # åˆ›å»ºé…ç½®
    config = Config()

    # åˆå§‹åŒ–é—®ç­”ç³»ç»Ÿ
    qa_system = DocumentQASystem(config)

    # åˆ›å»ºç›®å½•
    os.makedirs(config.KNOWLEDGE_BASE_PATH, exist_ok=True)
    os.makedirs(config.VECTOR_STORE_PATH, exist_ok=True)

    # ç¤ºä¾‹æ–‡æ¡£ï¼ˆéœ€è¦å…ˆåˆ›å»ºï¼‰
    sample_docs = [
        "data/knowledge/python-guide.txt",
        "data/knowledge/ml-basics.txt"
    ]

    print("\n1. å‡†å¤‡çŸ¥è¯†åº“...")
    if not Path(sample_docs[0]).exists():
        print(f"âš ï¸  ç¤ºä¾‹æ–‡æ¡£ä¸å­˜åœ¨: {sample_docs[0]}")
        print("è¯·å…ˆåˆ›å»ºç¤ºä¾‹æ–‡æ¡£å†è¿è¡Œ")
        return

    # æ·»åŠ çŸ¥è¯†åº“
    qa_system.add_knowledge_base(sample_docs)

    print("\n" + "=" * 60)
    print("ğŸ’¬ å¼€å§‹é—®ç­”ï¼ˆè¾“å…¥ 'quit' é€€å‡ºï¼‰")
    print("=" * 60)

    while True:
        question = input("\nğŸ¤” é—®é¢˜: ")

        if question.lower() == 'quit':
            break

        try:
            result = qa_system.query(question)
            print(f"\nğŸ’¡ å›ç­”: {result['answer']}")
            print(f"ğŸ“š æ¥æº: {', '.join(result['sources'])}")
        except Exception as e:
            print(f"âŒ é”™è¯¯: {e}")


if __name__ == "__main__":
    main()
