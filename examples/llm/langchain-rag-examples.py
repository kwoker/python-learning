"""
LangChain RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰ç¤ºä¾‹
æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ LangChain æ„å»ºæ£€ç´¢å¢å¼ºç”Ÿæˆåº”ç”¨
"""

import os
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from langchain_community.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import TextLoader
from langchain_community.document_loaders import DirectoryLoader

# ============================================================================
# ç¤ºä¾‹ 1: ç®€å•çš„ RAG æµç¨‹
# ============================================================================

def simple_rag_example():
    """ç®€å•çš„ RAG åº”ç”¨ç¤ºä¾‹"""
    print("=" * 60)
    print("ç¤ºä¾‹ 1: ç®€å• RAG åº”ç”¨")
    print("=" * 60)

    print("RAG (Retrieval-Augmented Generation) å·¥ä½œæµç¨‹ï¼š")
    print("""
1. æ–‡æ¡£åŠ è½½ â†’ 2. æ–‡æœ¬åˆ†å— â†’ 3. å‘é‡åŒ– â†’ 4. å­˜å‚¨ â†’ 5. æ£€ç´¢ â†’ 6. ç”Ÿæˆç­”æ¡ˆ

ä»£ç ç¤ºä¾‹ï¼š

# 1. å‡†å¤‡æ–‡æ¡£ï¼ˆå‡è®¾æœ‰æ–‡æœ¬æ–‡ä»¶ï¼‰
loader = TextLoader("knowledge.txt")
documents = loader.load()

# 2. æ–‡æœ¬åˆ†å—
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)
splits = text_splitter.split_documents(documents)

# 3. åˆ›å»ºå‘é‡å­˜å‚¨
embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_documents(splits, embeddings)

# 4. åˆ›å»ºæ£€ç´¢å™¨
retriever = vectorstore.as_retriever(search_kwargs={"k": 4})

# 5. åˆ›å»º QA é“¾
template = PromptTemplate(
    input_variables=["context", "question"],
    template='''åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼š

ä¸Šä¸‹æ–‡ï¼š
{context}

é—®é¢˜ï¼š{question}

è¯·æä¾›å‡†ç¡®ã€è¯¦ç»†çš„å›ç­”ã€‚'''
)

llm = ChatOpenAI(model="gpt-3.5-turbo")
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    chain_type="stuff",
    chain_type_kwargs={"prompt": template}
)

# 6. é—®ç­”
question = "æ–‡æ¡£ä¸­æåˆ°çš„å…³é”®æŠ€æœ¯æœ‰å“ªäº›ï¼Ÿ"
result = qa_chain({"query": question})
print(result["result"])
    """)


# ============================================================================
# ç¤ºä¾‹ 2: å¤šæ–‡æ¡£å¤„ç†
# ============================================================================

def multi_document_rag():
    """å¤šæ–‡æ¡£ RAG åº”ç”¨ç¤ºä¾‹"""
    print("=" * 60)
    print("ç¤ºä¾‹ 2: å¤šæ–‡æ¡£å¤„ç†")
    print("=" * 60)

    print("å¤„ç†å¤šä¸ªæ–‡æ¡£çš„ RAG åº”ç”¨ï¼š")
    print("""
# 1. åŠ è½½å¤šä¸ªæ–‡æ¡£
loader = DirectoryLoader(
    "knowledge_base/",
    glob="**/*.txt"  # åŠ è½½æ‰€æœ‰ txt æ–‡ä»¶
)
documents = loader.load()

# 2. æ–‡æœ¬åˆ†å—ï¼ˆæ ¹æ®æ–‡æ¡£ç±»å‹è°ƒæ•´ç­–ç•¥ï¼‰
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50,
    separators=["\\n\\n", "\\n", "ã€‚", "ï¼", "ï¼Ÿ"]
)
splits = text_splitter.split_documents(documents)

# 3. å‘é‡åŒ–å’Œå­˜å‚¨
embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_documents(splits, embeddings)

# 4. å¸¦è¿‡æ»¤çš„æ£€ç´¢
retriever = vectorstore.as_retriever(
    search_type="similarity",
    search_kwargs={
        "k": 5,
        "filter": {"source": "technical_docs"}  # è¿‡æ»¤ç‰¹å®šæ¥æº
    }
)
    """)


# ============================================================================
# ç¤ºä¾‹ 3: è‡ªå®šä¹‰æ£€ç´¢ç­–ç•¥
# ============================================================================

def custom_retrieval_example():
    """è‡ªå®šä¹‰æ£€ç´¢ç­–ç•¥ç¤ºä¾‹"""
    print("=" * 60)
    print("ç¤ºä¾‹ 3: è‡ªå®šä¹‰æ£€ç´¢")
    print("=" * 60)

    print("ä¸åŒçš„æ£€ç´¢ç­–ç•¥ï¼š")
    print("""
# 1. ç›¸ä¼¼åº¦æœç´¢ï¼ˆé»˜è®¤ï¼‰
retriever = vectorstore.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 4}
)

# 2. æœ€å¤§è¾¹é™…ç›¸å…³æ€§æœç´¢ï¼ˆMMRï¼‰
retriever = vectorstore.as_retriever(
    search_type="mmr",
    search_kwargs={
        "k": 4,
        "lambda_mult": 0.5  # æ§åˆ¶å¤šæ ·æ€§å’Œç›¸å…³æ€§å¹³è¡¡
    }
)

# 3. è¿‡æ»¤æœç´¢
retriever = vectorstore.as_retriever(
    search_kwargs={
        "k": 4,
        "filter": {
            "category": "æŠ€æœ¯æ–‡æ¡£",
            "date": {"$gte": "2024-01-01"}
        }
    }
)
    """)


# ============================================================================
# ç¤ºä¾‹ 4: RAG é“¾çš„ä¸åŒç±»å‹
# ============================================================================

def rag_chain_types():
    """RAG é“¾ç±»å‹ç¤ºä¾‹"""
    print("=" * 60)
    print("ç¤ºä¾‹ 4: RAG Chain ç±»å‹")
    print("=" * 60)

    print("å››ç§ RAG é“¾ç±»å‹ï¼š")
    print("""
1. Stuff Chainï¼ˆé»˜è®¤ï¼‰
   - å°†æ‰€æœ‰æ£€ç´¢æ–‡æ¡£å¡è¿›ä¸€ä¸ª prompt
   - ç®€å•å¿«é€Ÿï¼Œä½†å¯èƒ½è¶…å‡º token é™åˆ¶

   qa_chain = RetrievalQA.from_chain_type(
       llm=llm,
       retriever=retriever,
       chain_type="stuff"
   )

2. Refine Chain
   - é€ä¸ªå¤„ç†æ–‡æ¡£ï¼Œè¿­ä»£æ„å»ºç­”æ¡ˆ
   - é€‚åˆé•¿æ–‡æ¡£æ€»ç»“

   qa_chain = RetrievalQA.from_chain_type(
       llm=llm,
       retriever=retriever,
       chain_type="refine"
   )

3. Map Reduce Chain
   - å…ˆå¯¹æ¯ä¸ªæ–‡æ¡£å•ç‹¬æ€»ç»“ï¼Œå†ç»¼åˆç­”æ¡ˆ
   - é€‚åˆå¤šæ–‡æ¡£é—®ç­”

   qa_chain = RetrievalQA.from_chain_type(
       llm=llm,
       retriever=retriever,
       chain_type="map_reduce"
   )

4. Map Rerank Chain
   - å¯¹æ¯ä¸ªæ–‡æ¡£æ‰“åˆ†ï¼Œé€‰æ‹©æœ€ç›¸å…³çš„
   - é€‚åˆéœ€è¦ç²¾å‡†ç­”æ¡ˆçš„åœºæ™¯

   qa_chain = RetrievalQA.from_chain_type(
       llm=llm,
       retriever=retriever,
       chain_type="map_rerank"
   )
    """)


# ============================================================================
# ç¤ºä¾‹ 5: å®Œæ•´çš„ RAG åº”ç”¨ç±»
# ============================================================================

class RAGApplication:
    """RAG åº”ç”¨å®Œæ•´ç¤ºä¾‹ç±»"""

    def __init__(self, knowledge_base_path, model_name="gpt-3.5-turbo"):
        self.knowledge_base_path = knowledge_base_path
        self.model_name = model_name
        self.llm = None
        self.vectorstore = None
        self.qa_chain = None
        self._initialize()

    def _initialize(self):
        """åˆå§‹åŒ– RAG åº”ç”¨"""
        print("æ­£åœ¨åˆå§‹åŒ– RAG åº”ç”¨...")

        # åˆå§‹åŒ–ç»„ä»¶
        self.llm = ChatOpenAI(model=self.model_name, temperature=0)
        self.embeddings = OpenAIEmbeddings()

        # åŠ è½½å’Œåˆ†å—æ–‡æ¡£
        loader = DirectoryLoader(
            self.knowledge_base_path,
            glob="**/*.txt"
        )
        documents = loader.load()

        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=50
        )
        splits = text_splitter.split_documents(documents)

        # åˆ›å»ºå‘é‡å­˜å‚¨
        self.vectorstore = Chroma.from_documents(splits, self.embeddings)

        # åˆ›å»ºæ£€ç´¢å™¨
        retriever = self.vectorstore.as_retriever(
            search_kwargs={"k": 4}
        )

        # åˆ›å»º QA é“¾
        prompt = PromptTemplate(
            input_variables=["context", "question"],
            template="""åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼š

ä¸Šä¸‹æ–‡ï¼š
{context}

é—®é¢˜ï¼š{question}

è¯·æä¾›å‡†ç¡®ã€è¯¦ç»†çš„å›ç­”ã€‚å¦‚æœä¸Šä¸‹æ–‡æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯´æ˜ã€‚"""
        )

        self.qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            retriever=retriever,
            chain_type="stuff",
            chain_type_kwargs={"prompt": prompt},
            return_source_documents=True
        )

    def query(self, question):
        """æŸ¥è¯¢æ–¹æ³•"""
        result = self.qa_chain({"query": question})
        return {
            "answer": result["result"],
            "sources": [doc.metadata["source"] for doc in result["source_documents"]]
        }

    def add_document(self, file_path):
        """æ·»åŠ æ–°æ–‡æ¡£"""
        loader = TextLoader(file_path)
        documents = loader.load()

        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=50
        )
        splits = text_splitter.split_documents(documents)

        # æ·»åŠ åˆ°å‘é‡å­˜å‚¨
        self.vectorstore.add_documents(splits)


def rag_application_demo():
    """RAG åº”ç”¨ç±»æ¼”ç¤º"""
    print("=" * 60)
    print("ç¤ºä¾‹ 5: å®Œæ•´ RAG åº”ç”¨ç±»")
    print("=" * 60)

    print("""
class RAGApplication:
    '''å®Œæ•´çš„ RAG åº”ç”¨å®ç°'''

    def __init__(self, knowledge_base_path):
        self.knowledge_base_path = knowledge_base_path
        self._initialize()

    def _initialize(self):
        # åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶
        pass

    def query(self, question):
        '''æ‰§è¡ŒæŸ¥è¯¢'''
        result = self.qa_chain({"query": question})
        return {
            "answer": result["result"],
            "sources": result["source_documents"]
        }

    def add_document(self, file_path):
        '''åŠ¨æ€æ·»åŠ æ–‡æ¡£'''
        pass

# ä½¿ç”¨ç¤ºä¾‹ï¼š
rag_app = RAGApplication("knowledge_base/")
result = rag_app.query("Python çš„ç‰¹ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ")
print(result["answer"])
print("æ¥æº:", result["sources"])
    """)


if __name__ == "__main__":
    print("\nğŸ” LangChain RAG åº”ç”¨ç¤ºä¾‹\n")

    simple_rag_example()
    multi_document_rag()
    custom_retrieval_example()
    rag_chain_types()
    rag_application_demo()

    print("\n" + "=" * 60)
    print("ğŸ“¦ éœ€è¦å®‰è£…çš„ä¾èµ–ï¼š")
    print("pip install langchain langchain-openai")
    print("pip install langchain-community")
    print("pip install chromadb")
    print("=" * 60)
    print("\nğŸ’¡ æ¥ä¸‹æ¥å¯ä»¥å°è¯•ï¼š")
    print("1. åˆ›å»º knowledge_base/ ç›®å½•å¹¶æ”¾å…¥æ–‡æ¡£")
    print("2. è®¾ç½® OPENAI_API_KEY")
    print("3. è¿è¡Œç¤ºä¾‹ä»£ç ")
    print("4. æŸ¥çœ‹ ragflow-integration.py äº†è§£ RAGFlow é›†æˆ")
    print("=" * 60 + "\n")
