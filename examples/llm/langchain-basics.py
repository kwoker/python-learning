"""
LangChain åŸºç¡€ç¤ºä¾‹
æ¼”ç¤º LangChain çš„æ ¸å¿ƒç»„ä»¶ï¼šLLMã€Promptã€Chain
"""

from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain

# ============================================================================
# ç¤ºä¾‹ 1: åŸºç¡€ LLM è°ƒç”¨
# ============================================================================

def basic_llm_example():
    """åŸºç¡€ LLM è°ƒç”¨ç¤ºä¾‹"""
    print("=" * 60)
    print("ç¤ºä¾‹ 1: åŸºç¡€ LLM è°ƒç”¨")
    print("=" * 60)

    # åˆå§‹åŒ– LLMï¼ˆéœ€è¦è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡ï¼‰
    # llm = ChatOpenAI(
    #     model="gpt-3.5-turbo",
    #     temperature=0.7
    # )

    # å¦‚æœæ²¡æœ‰ API å¯†é’¥ï¼Œå¯ä»¥ä½¿ç”¨æ¨¡æ‹Ÿå“åº”
    print("LLM è°ƒç”¨ç¤ºä¾‹ï¼ˆéœ€è¦ API å¯†é’¥ï¼‰:")
    print("""
    llm = ChatOpenAI(
        model="gpt-3.5-turbo",
        temperature=0.7
    )

    response = llm.invoke("è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ")
    print(response.content)
    """)

    print("\næ³¨æ„: éœ€è¦è®¾ç½®ç¯å¢ƒå˜é‡ OPENAI_API_KEY")
    print("export OPENAI_API_KEY='your-api-key-here'\n")


# ============================================================================
# ç¤ºä¾‹ 2: Prompt Template ä½¿ç”¨
# ============================================================================

def prompt_template_example():
    """Prompt Template ä½¿ç”¨ç¤ºä¾‹"""
    print("=" * 60)
    print("ç¤ºä¾‹ 2: Prompt Template")
    print("=" * 60)

    # åˆ›å»ºæç¤ºè¯æ¨¡æ¿
    template = PromptTemplate(
        input_variables=["topic", "audience"],
        template="è¯·ç”¨ç®€å•çš„æ–¹å¼å‘{audience}è§£é‡Š{topic}ã€‚"
    )

    print("Prompt Template ç¤ºä¾‹:")
    print(template.format(topic="æœºå™¨å­¦ä¹ ", audience="åˆå­¦è€…"))

    # ç»“åˆ LLM ä½¿ç”¨ï¼ˆéœ€è¦ API å¯†é’¥ï¼‰
    print("""
    # å®Œæ•´ç¤ºä¾‹ï¼š
    llm = ChatOpenAI(model="gpt-3.5-turbo")
    chain = LLMChain(llm=llm, prompt=template)

    result = chain.run(topic="æœºå™¨å­¦ä¹ ", audience="åˆå­¦è€…")
    print(result)
    """)


# ============================================================================
# ç¤ºä¾‹ 3: LLMChain ç»„åˆä½¿ç”¨
# ============================================================================

def llm_chain_example():
    """LLMChain ä½¿ç”¨ç¤ºä¾‹"""
    print("=" * 60)
    print("ç¤ºä¾‹ 3: LLMChain")
    print("=" * 60)

    # åˆ›å»ºç¿»è¯‘é“¾
    translation_template = PromptTemplate(
        input_variables=["text", "language"],
        template="å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘æˆ{language}ï¼š{text}"
    )

    print("ç¿»è¯‘ Chain ç¤ºä¾‹:")
    print("""
    llm = ChatOpenAI(model="gpt-3.5-turbo")
    translation_chain = LLMChain(
        llm=llm,
        prompt=translation_template
    )

    result = translation_chain.run(
        text="Hello, how are you?",
        language="ä¸­æ–‡"
    )
    print(result)  # è¾“å‡º: ä½ å¥½ï¼Œä½ æ€ä¹ˆæ ·ï¼Ÿ
    """)


# ============================================================================
# ç¤ºä¾‹ 4: å¯¹è¯è®°å¿†
# ============================================================================

def conversation_memory_example():
    """å¯¹è¯è®°å¿†ç¤ºä¾‹"""
    print("=" * 60)
    print("ç¤ºä¾‹ 4: å¯¹è¯è®°å¿†")
    print("=" * 60)

    print("å¸¦è®°å¿†çš„å¯¹è¯ Chain ç¤ºä¾‹:")
    print("""
    llm = ChatOpenAI(model="gpt-3.5-turbo")
    memory = ConversationBufferMemory()

    conversation = ConversationChain(
        llm=llm,
        memory=memory,
        verbose=True
    )

    # ç¬¬ä¸€è½®å¯¹è¯
    conversation.predict(input="æˆ‘å«å¼ ä¸‰")

    # ç¬¬äºŒè½®å¯¹è¯ï¼ˆæ¨¡å‹ä¼šè®°ä½ä¹‹å‰çš„å¯¹è¯ï¼‰
    conversation.predict(input="æˆ‘çš„åå­—æ˜¯ä»€ä¹ˆï¼Ÿ")
    # è¾“å‡º: ä½ çš„åå­—æ˜¯å¼ ä¸‰
    """)


# ============================================================================
# ç¤ºä¾‹ 5: å®ç”¨å·¥å…·å‡½æ•°
# ============================================================================

def create_qa_chain():
    """åˆ›å»ºé—®ç­”é“¾çš„å®ç”¨å‡½æ•°"""
    print("=" * 60)
    print("ç¤ºä¾‹ 5: å®ç”¨å‡½æ•°")
    print("=" * 60)

    print("""
def create_qa_bot():
    '''åˆ›å»ºä¸€ä¸ªç®€å•çš„é—®ç­”æœºå™¨äºº'''
    template = PromptTemplate(
        input_variables=["question"],
        template="ä½œä¸ºä¸€ä¸ªä¸“ä¸šçš„åŠ©æ‰‹ï¼Œè¯·å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š\\n{question}"
    )

    llm = ChatOpenAI(model="gpt-3.5-turbo")
    qa_chain = LLMChain(llm=llm, prompt=template)

    return qa_chain

# ä½¿ç”¨ç¤ºä¾‹ï¼š
qa_bot = create_qa_bot()
answer = qa_bot.run(question="Python çš„ä¸»è¦ç‰¹ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ")
print(answer)
    """)


if __name__ == "__main__":
    print("\nğŸš€ LangChain åŸºç¡€ç¤ºä¾‹\n")

    basic_llm_example()
    prompt_template_example()
    llm_chain_example()
    conversation_memory_example()
    create_qa_chain()

    print("\n" + "=" * 60)
    print("ğŸ“š æ¥ä¸‹æ¥å¯ä»¥å°è¯•ï¼š")
    print("1. å®‰è£… langchain-openai: pip install langchain-openai")
    print("2. è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
    print("3. å–æ¶ˆæ³¨é‡Šä»£ç ä¸­çš„å®é™…è°ƒç”¨")
    print("4. æŸ¥çœ‹ langchain-rag-examples.py äº†è§£ RAG åº”ç”¨")
    print("=" * 60)
